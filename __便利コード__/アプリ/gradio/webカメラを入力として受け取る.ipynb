{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import gradio as gr\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "from diffusers.utils import load_image\n",
    "from controlnet_aux import OpenposeDetector\n",
    "\n",
    "class ControleNet_generator:\n",
    "    def __init__(   self, \n",
    "                    controlnet_model_dir, \n",
    "                    pretrained_model_name_or_path=\"stabilityai/stable-diffusion-2-1-base\",\n",
    "                    progress_bar_disable = True,\n",
    "                    ) -> None:\n",
    "        \n",
    "        self.controlnet = ControlNetModel.from_pretrained(str(controlnet_model_dir), torch_dtype=torch.float32)\n",
    "\n",
    "        self.pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "            pretrained_model_name_or_path=pretrained_model_name_or_path, \n",
    "            controlnet=self.controlnet, \n",
    "            safety_checker=None,\n",
    "            revision=None,\n",
    "            torch_dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        self.pipe.scheduler = UniPCMultistepScheduler.from_config(self.pipe.scheduler.config)\n",
    "        self.pipe = self.pipe.to(\"cuda\")\n",
    "        # self.pipe.enable_model_cpu_offload()\n",
    "\n",
    "        self.pipe.set_progress_bar_config(disable=progress_bar_disable)\n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "        return None\n",
    "\n",
    "    def generate(\n",
    "                self,\n",
    "                source_image:np.ndarray, \n",
    "                positive_prompt:str     = \"\", \n",
    "                negative_prompt:str     = \"low quality, worst quality, bad fingers, out of focus, bad face, extra arms, extra legs, blurry, bokeh, ugly\", \n",
    "                seed:int                = -1, \n",
    "                num_inference_steps:int = 20, \n",
    "                device:str              = \"cuda\", \n",
    "            ):\n",
    "        \n",
    "        if seed < 1:\n",
    "            generator = torch.Generator(device=device).manual_seed(random.randint(1, 10000000))\n",
    "        else:\n",
    "            generator = torch.Generator(device=device).manual_seed(seed)\n",
    "\n",
    "        with torch.autocast(\"cuda\"):\n",
    "            output = self.pipe(\n",
    "                                    prompt              = positive_prompt,\n",
    "                                    image               = Image.fromarray(source_image),\n",
    "                                    negative_prompt     = negative_prompt,\n",
    "                                    generator           = generator,\n",
    "                                    num_inference_steps = num_inference_steps,\n",
    "                                )\n",
    "        \n",
    "        return np.array(output.images[0])\n",
    "\n",
    "openpose_model = OpenposeDetector.from_pretrained(\"lllyasviel/ControlNet\")\n",
    "controlnet_model_dir = \"fusing/stable-diffusion-v1-5-controlnet-openpose\"\n",
    "controlnet_model = ControleNet_generator(str(controlnet_model_dir), pretrained_model_name_or_path=\"runwayml/stable-diffusion-v1-5\", progress_bar_disable=False)\n",
    "\n",
    "img_size = (512, 512)\n",
    "\n",
    "def get_pose_img(img):\n",
    "    img = cv2.resize(img, img_size)\n",
    "    pose_img = openpose_model(img)\n",
    "    pose_img = np.array(pose_img)\n",
    "    pose_img = cv2.resize(pose_img, img_size)\n",
    "    return pose_img\n",
    "\n",
    "def get_generated_img(pose_img, gen_prompt):\n",
    "    pose_img = cv2.resize(pose_img, img_size)\n",
    "    gen_img = controlnet_model.generate(pose_img, positive_prompt=gen_prompt, num_inference_steps=10)\n",
    "    return gen_img\n",
    "\n",
    "def process(img, gen_prompt=\"best quality\"):\n",
    "    img = cv2.resize(img, img_size)\n",
    "    pose_img = get_pose_img(img)\n",
    "    gen_img = get_generated_img(pose_img, gen_prompt)\n",
    "    return cv2.hconcat([pose_img, gen_img]), img\n",
    "\n",
    "\n",
    "examples = [\n",
    "                [None, \"super-hero character, powerful, masterpiece, best quality, extremely detailed\"], \n",
    "                [None, \"an astronaut on the moon, digital art\"], \n",
    "                [None, \"Dancing Darth Vader, best quality, extremely detailed\"]\n",
    "            ]\n",
    "\n",
    "demo = gr.Interface(\n",
    "    process, \n",
    "    # [   gr.Image(source=\"webcam\", streaming=True), \n",
    "    [   gr.Image(), \n",
    "        gr.inputs.Textbox(lines=2, label=\"Prompt\", default=\"super-hero character, powerful, masterpiece, best quality, extremely detailed\")\n",
    "    ], \n",
    "    [\"image\", \"image\"],\n",
    "    label=\"Prompt Example\",\n",
    "    examples=examples,\n",
    ")\n",
    "demo.launch(server_name=\"172.16.15.127\", server_port=9999, debug=True)\n",
    "# demo.launch(share=True)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiaug",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
